{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 2: Perceptrons\n",
    "\n",
    "Course 2024/25: *F. Chiariotti*\n",
    "\n",
    "The notebook contains a simple learning task over which we will implement **MODEL SELECTION AND VALIDATION**.\n",
    "\n",
    "Complete all the **required code sections** and **answer all the questions**.\n",
    "\n",
    "### IMPORTANT for the exam:\n",
    "\n",
    "The functions you might be required to implement in the exam will have the same signature and parameters as the ones in the labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Stayed/Churned Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Customer Churn table contains information on all 3,758 customers from a Telecommunications company in California in Q2 2022. Companies are naturally interested in churn, i.e., in which users are likely to switch to another company soon to get a better deal, and which are more loyal customers.\n",
    "\n",
    "The dataset contains three features:\n",
    "- **Tenure in Months**: Number of months the customer has stayed with the company\n",
    "- **Monthly Charge**: The amount charged to the customer monthly\n",
    "- **Age**: Customer's age\n",
    "\n",
    "The aim of the task is to predict if a customer will churn or not based on the three features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary Python libraries and load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "The dataset is a `.csv` file containing three input features and a label. Here is an example of the first 4 rows of the dataset: \n",
    "\n",
    "<center>\n",
    "\n",
    "Tenure in Months | Monthly Charge | Age | Customer Status |\n",
    "| -----------------| ---------------|-----|-----------------|\n",
    "| 9 | 65.6 | 37 | 0 |\n",
    "| 9 | -4.0 | 46 | 0 |\n",
    "| 4 | 73.9 | 50 | 1 |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "</center>\n",
    "\n",
    "Customer Status is 0 if the customer has stayed with the company and 1 if the customer has churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_dataset(filename):\n",
    "    data_train = pd.read_csv(filename)\n",
    "    #permute the data\n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True) # shuffle the data\n",
    "    X = data_train.iloc[:, 0:3].values # Get first two columns as the input\n",
    "    Y = data_train.iloc[:, 3].values # Get the third column as the label\n",
    "    Y = 2*Y-1 # Make sure labels are -1 or 1 (0 --> -1, 1 --> 1)\n",
    "    return X,Y\n",
    "\n",
    "# Load the dataset\n",
    "X, Y = load_dataset('data/telecom_customer_churn_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to differentiate (classify) between **class \"1\" (churned)** and **class \"-1\" (stayed)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the train set: 2817\n",
      "Number of samples in the test set: 940\n",
      "Number of churned users in test: 465\n",
      "Number of loyal users in test: 475\n",
      "Mean of the training input data: [ 0. -0. -0.]\n",
      "Std of the training input data: [1. 1. 1.]\n",
      "Mean of the test input data: [ 0.0134851   0.04850383 -0.0433016 ]\n",
      "Std of the test input data: [1.00014294 1.00683022 1.02078989]\n"
     ]
    }
   ],
   "source": [
    "# Compute the splits\n",
    "m_training = int(0.75*X.shape[0])\n",
    "\n",
    "# m_test is the number of samples in the test set (total-training)\n",
    "m_test =  X.shape[0] - m_training\n",
    "X_training =  X[:m_training]\n",
    "Y_training =  Y[:m_training]\n",
    "X_test =   X[m_training:]\n",
    "Y_test =  Y[m_training:]\n",
    "\n",
    "print(\"Number of samples in the train set:\", X_training.shape[0])\n",
    "print(\"Number of samples in the test set:\", X_test.shape[0])\n",
    "print(\"Number of churned users in test:\", np.sum(Y_test==-1))\n",
    "print(\"Number of loyal users in test:\", np.sum(Y_test==1))\n",
    "\n",
    "# Standardize the input matrix\n",
    "# The transformation is computed on training data and then used on all the 3 sets\n",
    "scaler = preprocessing.StandardScaler().fit(X_training) \n",
    "\n",
    "np.set_printoptions(suppress=True) # sets to zero floating point numbers < min_float_eps\n",
    "X_training =  scaler.transform(X_training)\n",
    "print (\"Mean of the training input data:\", X_training.mean(axis=0))\n",
    "print (\"Std of the training input data:\",X_training.std(axis=0))\n",
    "\n",
    "X_test =  scaler.transform(X_test)\n",
    "print (\"Mean of the test input data:\", X_test.mean(axis=0))\n",
    "print (\"Std of the test input data:\", X_test.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **homogeneous coordinates** to describe all the coefficients of the model.\n",
    "\n",
    "_Hint:_ The conversion can be performed with the function $hstack$ in $numpy$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_homogeneous(X_training, X_test):\n",
    "    # TODO: Transform the input into homogeneous coordinates\n",
    "    OneTrain = np.ones((X_training.shape[0], 1))\n",
    "    OneTest =  np.ones((X_test.shape[0], 1))\n",
    "\n",
    "    Xh_training = np.hstack((OneTrain, X_training))\n",
    "    Xh_test = np.hstack((OneTest, X_test))\n",
    "\n",
    "    return Xh_training, Xh_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set in homogeneous coordinates:\n",
      "[[ 1.          1.2361321   0.87798477 -0.16001986]\n",
      " [ 1.          0.10884685  0.4417593   1.37363294]\n",
      " [ 1.          1.69539647 -1.57223186 -0.04204657]\n",
      " [ 1.          0.15059816 -0.93544295  0.84275312]\n",
      " [ 1.          0.56811122 -0.38890759 -0.57292638]\n",
      " [ 1.         -0.39216881 -1.41010975 -0.39596645]\n",
      " [ 1.         -1.0184384  -1.53880462 -1.04481955]\n",
      " [ 1.         -0.35041751 -0.71649454  0.72477983]\n",
      " [ 1.         -1.18544362 -1.45857925  0.4298466 ]\n",
      " [ 1.          1.44488863 -1.4385229  -0.16001986]]\n"
     ]
    }
   ],
   "source": [
    "# convert to homogeneous coordinates using the function above\n",
    "X_training, X_test = to_homogeneous(X_training, X_test)\n",
    "print(\"Training set in homogeneous coordinates:\")\n",
    "print(X_training[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic perceptron\n",
    "\n",
    "Now **complete** the function *perceptron*. <br>\n",
    "The **perceptron** algorithm **does not terminate** if the **data** is not **linearly separable**, therefore your implementation should **terminate** if it **reached the termination** condition seen in class **or** if a **maximum number of iterations** have already been run, where one **iteration** corresponds to **one update of the perceptron weights**. In case the **termination** is reached **because** the **maximum** number of **iterations** have been completed, the implementation should **return the best model** seen throughout.\n",
    "\n",
    "The current version of the perceptron is **deterministic**: we use a fixed rule to decide which sample should be considered (e.g., the one with the lowest index).\n",
    "\n",
    "The input parameters to pass are:\n",
    "- $X$: the matrix of input features, one row for each sample\n",
    "- $Y$: the vector of labels for the input features matrix X\n",
    "- $max\\_num\\_iterations$: the maximum number of iterations for running the perceptron\n",
    "\n",
    "The output values are:\n",
    "- $best\\_w$: the vector with the coefficients of the best model (or the latest, if the termination condition is reached)\n",
    "- $best\\_error$: the *fraction* of misclassified samples for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_training\n",
    "curr_w = np.zeros(X.shape[1])\n",
    "\n",
    "n = 0\n",
    "index = []\n",
    "for i in range(10):\n",
    "   if np.sign(np.dot(np.transpose(X[i]), curr_w)) != Y[i]:\n",
    "      n += 1\n",
    "      index.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_errors(current_w, X, Y):\n",
    "    # This function:\n",
    "    # -computes the number of misclassified samples\n",
    "    # -returns the indexes of all misclassified samples\n",
    "    # -if there are no misclassified samples, returns -1 as index\n",
    "    # TODO: write the function\n",
    "\n",
    "    index = []\n",
    "    n = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        if np.sign(np.dot(np.transpose(X[i]), current_w)) != Y[i]:\n",
    "            n += 1\n",
    "            index.append(i)\n",
    "\n",
    "    if n == 0:\n",
    "        return 0, -1\n",
    "\n",
    "    else:\n",
    "        return n, index\n",
    "\n",
    "    \n",
    "def perceptron_fixed_update(current_w, X, Y):\n",
    "    # TODO: write the perceptron update function\n",
    "\n",
    "    new_w = current_w + Y*X\n",
    "    return new_w\n",
    "\n",
    "def perceptron_no_randomization(X, Y, max_num_iterations):\n",
    "    # TODO: write the perceptron main loop\n",
    "    # The perceptron should run for up to max_num_iterations, or stop if it finds a solution with ERM=0\n",
    "\n",
    "    curr_w = np.zeros(X.shape[1])\n",
    "    for i in range(max_num_iterations):\n",
    "\n",
    "        n, idx = count_errors(curr_w, X, Y)\n",
    "\n",
    "        if n == 0:\n",
    "            print('ERM condition reached')\n",
    "            best_w = curr_w\n",
    "            return best_w, 0\n",
    "        \n",
    "        else:\n",
    "            FirstMissX, FirstMissY = X[idx[0]], Y[idx[0]]\n",
    "            new_w = perceptron_fixed_update(curr_w, FirstMissX, FirstMissY)\n",
    "\n",
    "            curr_w = new_w\n",
    "\n",
    "    best_w = curr_w\n",
    "    best_error, _ = count_errors(best_w, X, Y)\n",
    "    return best_w, best_error/len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the implementation above of the perceptron to learn a model from the training data using 30 iterations and print the error of the best model we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error of perceptron (30 iterations): 0.2946396876109336\n",
      "Training Error of perceptron (100 iterations): 0.28718494852680154\n"
     ]
    }
   ],
   "source": [
    "w_found, error = perceptron_no_randomization(X_training,Y_training, 30)\n",
    "print(\"Training Error of perceptron (30 iterations): \" + str(error))\n",
    "w_found2, error2 = perceptron_no_randomization(X_training,Y_training, 100)\n",
    "print(\"Training Error of perceptron (100 iterations): \" + str(error2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the best model $w\\_found$ to **predict the labels for the test dataset** and print the fraction of misclassified samples in the test set (the test error that is an estimate of the true loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't_loss_estimate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_estimate\u001b[39m(w,X,Y):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Estimate the test loss\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t_loss_estimate\n\u001b[0;32m----> 6\u001b[0m true_loss_estimate \u001b[38;5;241m=\u001b[39m  loss_estimate(w_found, X_test, Y_test)       \u001b[38;5;66;03m# Error rate on the test set\u001b[39;00m\n\u001b[1;32m      7\u001b[0m true_loss_estimate2 \u001b[38;5;241m=\u001b[39m  loss_estimate(w_found2, X_test, Y_test) \n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Error of perceptron (30 iterations): \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(true_loss_estimate))\n",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m, in \u001b[0;36mloss_estimate\u001b[0;34m(w, X, Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_estimate\u001b[39m(w,X,Y):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Estimate the test loss\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t_loss_estimate\n",
      "\u001b[0;31mNameError\u001b[0m: name 't_loss_estimate' is not defined"
     ]
    }
   ],
   "source": [
    "def loss_estimate(w,X,Y):\n",
    "    # Estimate the test loss\n",
    "    return t_loss_estimate\n",
    "\n",
    "\n",
    "true_loss_estimate =  loss_estimate(w_found, X_test, Y_test)       # Error rate on the test set\n",
    "true_loss_estimate2 =  loss_estimate(w_found2, X_test, Y_test) \n",
    "    \n",
    "print(\"Test Error of perceptron (30 iterations): \" + str(true_loss_estimate))\n",
    "print(\"Test Error of perceptron (100 iterations): \" + str(true_loss_estimate2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized perceptron\n",
    "\n",
    "Implement the correct randomized version of the perceptron such that at each iteration the algorithm picks a random misclassified sample and updates the weights using that sample. The functions will be very similar, except for some minor details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_randomized_update(current_w, X, Y):\n",
    "    # TODO: write the perceptron update function\n",
    "    return new_w\n",
    "\n",
    "def perceptron_with_randomization(X, Y, max_num_iterations):\n",
    "    # TODO: write the perceptron main loop\n",
    "    # The perceptron should run for up to max_num_iterations, or stop if it finds a solution with ERM=0\n",
    "    return best_w, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the correct version of the perceptron using 30 iterations and print the error of the best model we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the perceptron for 30 iterations\n",
    "w_found, error = perceptron_with_randomization(X_training, Y_training, 30)\n",
    "w_found2, error2 = perceptron_with_randomization(X_training, Y_training, 100)\n",
    "print(\"Training Error of perceptron (30 iterations): \" + str(error))\n",
    "print(\"Training Error of perceptron (100 iterations): \" + str(error2))\n",
    "\n",
    "true_loss_estimate =  loss_estimate(w_found, X_test, Y_test)       # Error rate on the test set\n",
    "true_loss_estimate2 =  loss_estimate(w_found2, X_test, Y_test) \n",
    "\n",
    "print(\"Test Error of perceptron (30 iterations): \" + str(true_loss_estimate))\n",
    "print(\"Test Error of perceptron (100 iterations): \" + str(true_loss_estimate2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Plot the loss with respect to the number of iterations (up to 1000)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
