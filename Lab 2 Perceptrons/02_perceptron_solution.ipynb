{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 2: Perceptrons\n",
    "\n",
    "Course 2024/25: *F. Chiariotti*\n",
    "\n",
    "The notebook contains a simple learning task over which we will implement **MODEL SELECTION AND VALIDATION**.\n",
    "\n",
    "Complete all the **required code sections**.\n",
    "\n",
    "### IMPORTANT for the exam:\n",
    "\n",
    "The functions you might be required to implement in the exam will have the same signature and parameters as the ones in the labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Stayed/Churned Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Customer Churn table contains information on all 3,758 customers from a Telecommunications company in California in Q2 2022. Companies are naturally interested in churn, i.e., in which users are likely to switch to another company soon to get a better deal, and which are more loyal customers.\n",
    "\n",
    "The dataset contains three features:\n",
    "- **Tenure in Months**: Number of months the customer has stayed with the company\n",
    "- **Monthly Charge**: The amount charged to the customer monthly\n",
    "- **Age**: Customer's age\n",
    "\n",
    "The aim of the task is to predict if a customer will churn or not based on the three features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary Python libraries and load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "The dataset is a `.csv` file containing three input features and a label. Here is an example of the first 4 rows of the dataset: \n",
    "\n",
    "<center>\n",
    "\n",
    "Tenure in Months | Monthly Charge | Age | Customer Status |\n",
    "| -----------------| ---------------|-----|-----------------|\n",
    "| 9 | 65.6 | 37 | 0 |\n",
    "| 9 | -4.0 | 46 | 0 |\n",
    "| 4 | 73.9 | 50 | 1 |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "</center>\n",
    "\n",
    "Customer Status is 0 if the customer has stayed with the company and 1 if the customer has churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def load_dataset(filename):\n",
    "    data_train = pd.read_csv(filename)\n",
    "    #permute the data\n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True) # shuffle the data\n",
    "    X = data_train.iloc[:, 0:3].values # Get first two columns as the input\n",
    "    Y = data_train.iloc[:, 3].values # Get the third column as the label\n",
    "    Y = 2*Y-1 # Make sure labels are -1 or 1 (0 --> -1, 1 --> 1)\n",
    "    return X,Y\n",
    "\n",
    "# Load the dataset\n",
    "X, Y = load_dataset('data/telecom_customer_churn_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to differentiate (classify) between **class \"1\" (churned)** and **class \"-1\" (stayed)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the train set: 2817\n",
      "Number of samples in the test set: 940\n",
      "Number of churned users in test: 479\n",
      "Number of loyal users in test: 461\n",
      "Mean of the training input data: [-0.  0. -0.]\n",
      "Std of the training input data: [1. 1. 1.]\n",
      "Mean of the test input data: [0.0575483  0.05550169 0.0073833 ]\n",
      "Std of the test input data: [0.98593187 0.97629659 1.00427583]\n"
     ]
    }
   ],
   "source": [
    "# Compute the splits\n",
    "m_training = int(0.75*X.shape[0])\n",
    "\n",
    "# m_test is the number of samples in the test set (total-training)\n",
    "m_test =  X.shape[0] - m_training\n",
    "X_training =  X[:m_training]\n",
    "Y_training =  Y[:m_training]\n",
    "X_test =   X[m_training:]\n",
    "Y_test =  Y[m_training:]\n",
    "\n",
    "print(\"Number of samples in the train set:\", X_training.shape[0])\n",
    "print(\"Number of samples in the test set:\", X_test.shape[0])\n",
    "print(\"Number of churned users in test:\", np.sum(Y_test==-1))\n",
    "print(\"Number of loyal users in test:\", np.sum(Y_test==1))\n",
    "\n",
    "# Standardize the input matrix\n",
    "# The transformation is computed on training data and then used on all the 3 sets\n",
    "scaler = preprocessing.StandardScaler().fit(X_training) \n",
    "\n",
    "np.set_printoptions(suppress=True) # sets to zero floating point numbers < min_float_eps\n",
    "X_training =  scaler.transform(X_training)\n",
    "print (\"Mean of the training input data:\", X_training.mean(axis=0))\n",
    "print (\"Std of the training input data:\",X_training.std(axis=0))\n",
    "\n",
    "X_test =  scaler.transform(X_test)\n",
    "print (\"Mean of the test input data:\", X_test.mean(axis=0))\n",
    "print (\"Std of the test input data:\", X_test.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **homogeneous coordinates** to describe all the coefficients of the model.\n",
    "\n",
    "_Hint:_ The conversion can be performed with the function $hstack$ in $numpy$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_homogeneous(X_training, X_test):\n",
    "    Xh_training = np.hstack([np.ones( (X_training.shape[0], 1) ), X_training])\n",
    "    Xh_test = np.hstack([np.ones( (X_test.shape[0], 1) ), X_test])\n",
    "    return Xh_training, Xh_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set in homogeneous coordinates:\n",
      "[[ 1.         -0.3798618  -1.57020044  0.85174963]\n",
      " [ 1.         -0.87925308  0.47180292  1.08667766]\n",
      " [ 1.         -0.75440526 -0.6130632  -0.26415851]\n",
      " [ 1.         -1.12894873  0.09856916 -0.96894261]\n",
      " [ 1.         -1.12894873 -0.58486332 -1.20387064]\n",
      " [ 1.          1.78416712  1.39908145  0.08823353]\n",
      " [ 1.         -0.7960212  -1.0990965  -0.32289052]\n",
      " [ 1.          0.20276137 -0.39907585 -0.96894261]\n",
      " [ 1.         -0.62955744  0.63934341  0.96921364]\n",
      " [ 1.         -0.87925308  1.13201197 -0.02923048]]\n"
     ]
    }
   ],
   "source": [
    "# convert to homogeneous coordinates using the function above\n",
    "X_training, X_test = to_homogeneous(X_training, X_test)\n",
    "print(\"Training set in homogeneous coordinates:\")\n",
    "print(X_training[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic perceptron\n",
    "\n",
    "Now **complete** the function *perceptron*. <br>\n",
    "The **perceptron** algorithm **does not terminate** if the **data** is not **linearly separable**, therefore your implementation should **terminate** if it **reached the termination** condition seen in class **or** if a **maximum number of iterations** have already been run, where one **iteration** corresponds to **one update of the perceptron weights**. In case the **termination** is reached **because** the **maximum** number of **iterations** have been completed, the implementation should **return the best model** seen throughout.\n",
    "\n",
    "The current version of the perceptron is **deterministic**: we use a fixed rule to decide which sample should be considered (e.g., the one with the lowest index).\n",
    "\n",
    "The input parameters to pass are:\n",
    "- $X$: the matrix of input features, one row for each sample\n",
    "- $Y$: the vector of labels for the input features matrix X\n",
    "- $max\\_num\\_iterations$: the maximum number of iterations for running the perceptron\n",
    "\n",
    "The output values are:\n",
    "- $best\\_w$: the vector with the coefficients of the best model (or the latest, if the termination condition is reached)\n",
    "- $best\\_error$: the *fraction* of misclassified samples for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_errors(current_w, X, Y):\n",
    "    # Find all indices which have a different sign from the corresponding labels\n",
    "    index = np.nonzero(np.sign(np.dot(X, current_w)) - Y)[0]\n",
    "    n = np.array(index).shape[0]\n",
    "    if (n == 0):\n",
    "        # There are no misclassified samples\n",
    "        return 0, -1\n",
    "    return n, index\n",
    "\n",
    "    \n",
    "def perceptron_fixed_update(current_w, X, Y):\n",
    "    new_w=current_w\n",
    "    n, idx = count_errors(current_w, X, Y)\n",
    "    # Choose the first misclassified sample\n",
    "    if (n > 0):\n",
    "        new_w = current_w + Y[idx[0]] * X[idx[0],:]    \n",
    "    return new_w\n",
    "\n",
    "def perceptron_no_randomization(X, Y, max_num_iterations):\n",
    "    \n",
    "    # Initialize some support variables\n",
    "    num_samples = X.shape[0]\n",
    "    num_iter = 0\n",
    "    \n",
    "    # Initialize the weights of the algorith with w=0\n",
    "    curr_w = np.zeros(X.shape[1])\n",
    "\n",
    "    # Compute the number of misclassified samples and the indexes\n",
    "    num_misclassified, index_misclassified = count_errors(curr_w, X, Y)\n",
    "    # Initialize the best model\n",
    "    best_error = num_misclassified / num_samples\n",
    "    best_w = curr_w.copy()\n",
    "    \n",
    "    # Main loop: continue until all samples correctly classified or max # iterations reached\n",
    "    # Remember: if no errors were found, index_misclassified = -1\n",
    "    while num_misclassified != 0 and num_iter < max_num_iterations:\n",
    "        # Update the perceptron\n",
    "        curr_w = perceptron_fixed_update(curr_w, X, Y)\n",
    "        # Find the new error\n",
    "        num_misclassified, index_misclassified = count_errors(curr_w, X, Y)\n",
    "        if num_misclassified / num_samples < best_error:\n",
    "            best_error = num_misclassified / num_samples\n",
    "            best_w = curr_w.copy()\n",
    "        num_iter += 1\n",
    "    \n",
    "    return best_w, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the implementation above of the perceptron to learn a model from the training data using 30 iterations and print the error of the best model we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error of perceptron (30 iterations): 0.2751153709620163\n",
      "Training Error of perceptron (100 iterations): 0.2751153709620163\n"
     ]
    }
   ],
   "source": [
    "# Now run the perceptron for 100 iterations\n",
    "w_found, error = perceptron_no_randomization(X_training,Y_training, 30)\n",
    "print(\"Training Error of perceptron (30 iterations): \" + str(error))\n",
    "w_found2, error2 = perceptron_no_randomization(X_training,Y_training, 100)\n",
    "print(\"Training Error of perceptron (100 iterations): \" + str(error2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the best model $w\\_found$ to **predict the labels for the test dataset** and print the fraction of misclassified samples in the test set (the test error that is an estimate of the true loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error of perceptron (30 iterations): 0.26382978723404255\n",
      "Test Error of perceptron (100 iterations): 0.26382978723404255\n"
     ]
    }
   ],
   "source": [
    "def loss_estimate(w, X, Y):\n",
    "    err,_ = count_errors(w, X, Y)\n",
    "    return err / len(Y)\n",
    "\n",
    "true_loss_estimate =  loss_estimate(w_found, X_test, Y_test)      # Error rate on the test set\n",
    "true_loss_estimate2 =  loss_estimate(w_found2, X_test, Y_test) \n",
    "    \n",
    "print(\"Test Error of perceptron (30 iterations): \" + str(true_loss_estimate))\n",
    "print(\"Test Error of perceptron (100 iterations): \" + str(true_loss_estimate2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.1.4):** implement the correct randomized version of the perceptron such that at each iteration the algorithm picks a random misclassified sample and updates the weights using that sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_randomized_update(current_w, X, Y):\n",
    "    new_w=current_w\n",
    "    n, idx = count_errors(current_w, X, Y)\n",
    "    # Choose a random misclassified sample\n",
    "    if (n > 0):\n",
    "        chosen = rnd.sample(sorted(idx), 1)[0]\n",
    "        new_w = current_w + Y[chosen] * X[chosen,:]\n",
    "    return new_w\n",
    "\n",
    "def perceptron_with_randomization(X, Y, max_num_iterations):\n",
    "    \n",
    "    # Initialize some support variables\n",
    "    num_samples = X.shape[0]\n",
    "    num_iter = 0\n",
    "    \n",
    "    # Initialize the weights of the algorith with w=0\n",
    "    curr_w = np.zeros(X.shape[1])\n",
    "\n",
    "    # Compute the number of misclassified samples and the indexes\n",
    "    num_misclassified, index_misclassified = count_errors(curr_w, X, Y)\n",
    "    # Initialize the best model\n",
    "    best_error = num_misclassified / num_samples\n",
    "    best_w = curr_w.copy()\n",
    "    \n",
    "    # Main loop: continue until all samples correctly classified or max # iterations reached\n",
    "    # Remember: if no errors were found, index_misclassified = -1\n",
    "    while num_misclassified != 0 and num_iter < max_num_iterations:\n",
    "        # Update the perceptron\n",
    "        curr_w = perceptron_randomized_update(curr_w, X, Y)\n",
    "        # Find the new error\n",
    "        num_misclassified, index_misclassified = count_errors(curr_w, X, Y)\n",
    "        if num_misclassified / num_samples < best_error:\n",
    "            best_error = num_misclassified / num_samples\n",
    "            best_w = curr_w.copy()\n",
    "        num_iter += 1\n",
    "    \n",
    "    return best_w, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the correct version of the perceptron using 30 iterations and print the error of the best model we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error of perceptron (30 iterations): 0.24600638977635783\n",
      "Training Error of perceptron (100 iterations): 0.24742634007809727\n",
      "Test Error of perceptron (30 iterations): 0.24361702127659574\n",
      "Test Error of perceptron (100 iterations): 0.24680851063829787\n"
     ]
    }
   ],
   "source": [
    "# Now run the perceptron for 30 iterations\n",
    "w_found, error = perceptron_with_randomization(X_training, Y_training, 30)\n",
    "w_found2, error2 = perceptron_with_randomization(X_training, Y_training, 100)\n",
    "print(\"Training Error of perceptron (30 iterations): \" + str(error))\n",
    "print(\"Training Error of perceptron (100 iterations): \" + str(error2))\n",
    "\n",
    "true_loss_estimate =  loss_estimate(w_found, X_test, Y_test)     # Error rate on the test set\n",
    "true_loss_estimate2 =  loss_estimate(w_found2, X_test, Y_test) \n",
    "    \n",
    "print(\"Test Error of perceptron (30 iterations): \" + str(true_loss_estimate))\n",
    "print(\"Test Error of perceptron (100 iterations): \" + str(true_loss_estimate2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_iter \u001b[38;5;129;01min\u001b[39;00m num_iters:\n\u001b[1;32m      9\u001b[0m     _, error_rand \u001b[38;5;241m=\u001b[39m perceptron_with_randomization(X_training, Y_training, num_iter)\n\u001b[0;32m---> 10\u001b[0m     _, error_det \u001b[38;5;241m=\u001b[39m perceptron_no_randomization(X_training, Y_training, num_iter)\n\u001b[1;32m     11\u001b[0m     errors_rand\u001b[38;5;241m.\u001b[39mappend(error_rand)\n\u001b[1;32m     12\u001b[0m     errors_det\u001b[38;5;241m.\u001b[39mappend(error_det)\n",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m, in \u001b[0;36mperceptron_no_randomization\u001b[0;34m(X, Y, max_num_iterations)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Main loop: continue until all samples correctly classified or max # iterations reached\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Remember: if no errors were found, index_misclassified = -1\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m num_misclassified \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_iter \u001b[38;5;241m<\u001b[39m max_num_iterations:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Update the perceptron\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     curr_w \u001b[38;5;241m=\u001b[39m perceptron_fixed_update(curr_w, X, Y)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Find the new error\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     num_misclassified, index_misclassified \u001b[38;5;241m=\u001b[39m count_errors(curr_w, X, Y)\n",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mperceptron_fixed_update\u001b[0;34m(current_w, X, Y)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperceptron_fixed_update\u001b[39m(current_w, X, Y):\n\u001b[1;32m     12\u001b[0m     new_w\u001b[38;5;241m=\u001b[39mcurrent_w\n\u001b[0;32m---> 13\u001b[0m     n, idx \u001b[38;5;241m=\u001b[39m count_errors(current_w, X, Y)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Choose the first misclassified sample\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m, in \u001b[0;36mcount_errors\u001b[0;34m(current_w, X, Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_errors\u001b[39m(current_w, X, Y):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Find all indices which have a different sign from the corresponding labels\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(np\u001b[38;5;241m.\u001b[39msign(np\u001b[38;5;241m.\u001b[39mdot(X, current_w)) \u001b[38;5;241m-\u001b[39m Y)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m     n \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(index)\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# There are no misclassified samples\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss with respect to the number of iterations\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "num_iters = np.arange(0, 1001,20)\n",
    "errors_rand = []\n",
    "errors_det = []\n",
    "\n",
    "for num_iter in num_iters:\n",
    "    _, error_rand = perceptron_with_randomization(X_training, Y_training, num_iter)\n",
    "    _, error_det = perceptron_no_randomization(X_training, Y_training, num_iter)\n",
    "    errors_rand.append(error_rand)\n",
    "    errors_det.append(error_det)\n",
    "\n",
    "plt.plot(num_iters, errors_rand, label='Random')\n",
    "plt.plot(num_iters, errors_det, label='Deterministic')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Training error')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
