{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 3: LINEAR REGRESSION\n",
    "\n",
    "Course 2024/25: *F. Chiariotti*\n",
    "\n",
    "The notebook contains some simple tasks to be performed with **LINEAR REGRESSION MODELS**.\n",
    "\n",
    "Complete all the **required code sections**.\n",
    "\n",
    "### IMPORTANT for the exam:\n",
    "\n",
    "The functions you might be required to implement in the exam will have the same signature and parameters as the ones in the labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VR traffic prediction\n",
    "\n",
    "In this notebook, we will explore the prediction of Virtual Reality (VR) traffic. The data come from the paper:\n",
    "\n",
    "Lecci, Mattia, _et al._ \"An open framework for analyzing and modeling XR network traffic.\" IEEE Access 9 (2021): 129782-129795.\n",
    "\n",
    "The VR game Virus Popper was instantiated on a computer through the RiftCat application: the user could then see the virtual content on their phone, which was strapped to their head with a Cardboard viewer. The file virus_popper.csv contains three columns from the traffic capture:\n",
    "idx    | frame size (B) |  time (s)\n",
    "0      | 38424          |  0.0\n",
    "1      | 39801          |  0.01944\n",
    "...\n",
    "\n",
    "The game was run at 60 frames per second, with a target rate of 30 Mb/s. The task is then to predict the size of the next frame, given the past N frames. This was explored in another paper:\n",
    "\n",
    "Chiariotti, Federico, _et al._ \"Temporal characterization and prediction of vr traffic: A network slicing use case.\" IEEE Transactions on Mobile Computing 23.5 (2023): 3890-3908.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def load_dataset(filename):\n",
    "    data_train = pd.read_csv(filename)\n",
    "    data = data_train.iloc[:, 1].values # Get the second column (frame size) as the input\n",
    "    return data\n",
    "\n",
    "# Load the dataset\n",
    "data = load_dataset('data/virus_popper.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data and create training and test sets\n",
    "\n",
    "In this case, we are learning a time series: let us consider a memory of 2 samples, i.e., use X[n-1] and X[n-2] to predict X[n]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25878, 3) (25878,) (8625, 3) (8625,)\n"
     ]
    }
   ],
   "source": [
    "# Normalize the dataset\n",
    "avg_size = np.mean(data)\n",
    "norm_data = np.asarray(data) / avg_size\n",
    "\n",
    "# Compute the splits and prepare the columns\n",
    "m_training = int(0.75*norm_data.shape[0])\n",
    "\n",
    "X_training = np.ones([m_training - 2, 3])\n",
    "X_training[:,1] = norm_data[: m_training - 2]\n",
    "X_training[:,2] = norm_data[1 : m_training - 1]\n",
    "Y_training = norm_data[2 : m_training]\n",
    "\n",
    "\n",
    "X_test = np.ones([norm_data.shape[0] - m_training - 2, 3])\n",
    "X_test[:,1] = norm_data[m_training : -2]\n",
    "X_test[:,2] = norm_data[m_training + 1 : -1]\n",
    "Y_test = norm_data[m_training + 2:]\n",
    "\n",
    "print(np.shape(X_training), np.shape(Y_training), np.shape(X_test), np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares linear regression\n",
    "\n",
    "Train and evaluate the LS regressor on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00163978, -0.00080261, -0.00080291],\n",
       "       [-0.00080261,  0.00172933, -0.0009245 ],\n",
       "       [-0.00080291, -0.0009245 ,  0.00172958]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.dot(X_training.T, X_training)\n",
    "\n",
    "A = np.linalg.inv(A)\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least squares solution\n",
    "def least_squares(X_matrix: np.ndarray, labels: np.ndarray) -> None:\n",
    "    ## TODO: Run the LS algorithm without regularization\n",
    "\n",
    "    A = np.dot(X_matrix.T, X_matrix)\n",
    "    b = np.dot(X_matrix.T, labels)\n",
    "    w = np.dot(np.linalg.inv(A), b)\n",
    "\n",
    "    return w\n",
    "    \n",
    "\n",
    "def evaluate_model(x, y, coeff):\n",
    "    ## TODO: Return the average MSE for the set over which we evaluate\n",
    "    mse = np.sum((np.dot(x, coeff.T) - y)**2)/len(y)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 9070.165039219852\n",
      "Model coefficients: [0.36986742 0.20348677 0.42565451]\n",
      "Root MSE: 7712.354721539114\n"
     ]
    }
   ],
   "source": [
    "# Run the LS training and test it on the test data\n",
    "trained_model = least_squares(X_training, Y_training)\n",
    "print('Training RMSE:', np.sqrt(evaluate_model(X_training, Y_training, trained_model)) * avg_size)\n",
    "mse = evaluate_model(X_test, Y_test, trained_model)\n",
    "print('Model coefficients:', trained_model)\n",
    "print('Root MSE:', np.sqrt(mse) * avg_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares with Tikhonov regularization\n",
    "\n",
    "Perform K-fold cross validation with $\\lambda\\in\\{0, 0.1, 1, 10\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.eye(10) * 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least squares solution\n",
    "def regularized_least_squares(X_matrix: np.ndarray, labels: np.ndarray, lambda_par: np.ndarray) -> None:\n",
    "    # Create A and b as in unregularized LS\n",
    "    A = np.transpose(X_matrix).dot(X_matrix)\n",
    "    b = np.transpose(X_matrix).dot(labels)\n",
    "    # Add the regularization part in the inverse\n",
    "    coeff = np.linalg.inv(A + lambda_par * np.eye(np.shape(A)[0])).dot(b)\n",
    "    return coeff\n",
    "\n",
    "    \n",
    "\n",
    "def K_fold(X_training: np.ndarray, Y_training: np.ndarray, lambda_vec: np.ndarray, K: np.ndarray) -> None:\n",
    "    results = []\n",
    "\n",
    "    #Ensures that does not go above the number of train data\n",
    "    #! IMPORTANT\n",
    "\n",
    "    fracData = int(np.floor(len(X_training)/K))\n",
    "\n",
    "    for l in lambda_vec:\n",
    "\n",
    "        ErrorForFold = []\n",
    "\n",
    "        for i in range(K):\n",
    "\n",
    "            #separate train into train and validation\n",
    "            idx = np.arange(len(X_training))\n",
    "            #! IMPORTANT\n",
    "            valIdx = np.arange(fracData) + i*fracData\n",
    "\n",
    "            trainIdx = [i for i in idx if i not in valIdx]\n",
    "            \n",
    "            xVal, yVal = X_training[valIdx], Y_training[valIdx]\n",
    "            xTrain, yTrain = X_training[trainIdx], Y_training[trainIdx]\n",
    "\n",
    "            FitCoeff = regularized_least_squares(xTrain, yTrain, l)\n",
    "            mse = evaluate_model(xVal, yVal, FitCoeff)\n",
    "\n",
    "            ErrorForFold.append(mse)\n",
    "\n",
    "\n",
    "        results.append(np.mean(ErrorForFold))\n",
    "    \n",
    "    bestRes = np.min(results)\n",
    "    bestLambda = lambda_vec[np.argmin(results)]\n",
    "\n",
    "    best = (bestRes, bestLambda)\n",
    "\n",
    "\n",
    "    return best, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training with K-fold cross-validation and plot the score\n",
    "K = 5\n",
    "lambda_par = range(100)\n",
    "\n",
    "best_model, results = K_fold(X_training, Y_training, lambda_par, K)\n",
    "print(best_model, results)\n",
    "plt.plot(lambda_par, np.sqrt(results) * avg_size)\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results for the regularized models on the test set\n",
    "test_scores = np.zeros(len(models))\n",
    "\n",
    "for i in range(len(models)):\n",
    "    test_scores[i] = evaluate_model(X_test, Y_test, models[i])\n",
    "\n",
    "plt.plot(lambda_par, np.sqrt(test_scores) * avg_size)\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA\n",
    "\n",
    "Can you figure out the best amount of memory to use?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
